---
title: "Investigating Optimizer-Induced Implicit Bias in Transformers for NLP Tasks"
excerpt: "*Keywords: Self-attention Mechanism, Next Token Prediction, Adam/AdamW, optimization*<br/><img src='/images/IMG_3105.png' width='450'>"
collection: publications
---
{% include base_path %}
*Keywords: Self-attention Mechanism, Next Token Prediction, Adam/AdamW, optimization*

**Advisors:** Prof. [Yingbin Liang](https://sites.google.com/view/yingbinliang/home), Ohio State University

**Project Overview**

- Extending current work on implicit biases in transformers by providing theoretical analyses of training dynamics in specific NLP tasks, such as next token prediction.
- Investigating optimizer-induced biases of Adam/AdamW, moving beyond empirical observations to provide theoretical insights into their advantages and efficiency in transformer models.
- Relaxing existing assumptions in the current work to broaden the theoretical understanding of transformer training, with a focus on practical implications in NLP tasks.

[[slides (for a pre during the internship)]](files/r1)
