---
title: "Convergence of AdamW Under Relaxed Assumptions"
excerpt: "*Keywords: Generalized smoothness; Adam/AdamW; Non-convex optimization; Stochastic optimization*<br/><img src='/images/IMG_3105.png' width='450'>"
collection: publications
---

{% include base_path %}
*Keywords: Generalized smoothness; Adam/AdamW; Non-convex optimization; Stochastic optimization*

**Advisors:** Prof. [Yingbin Liang](https://sites.google.com/view/yingbinliang/home), Ohio State University

**Project Overview**

- Developed analysis of theoretical properties of AdamW without unrealistically strong assumptions, especially without 1) Globally bounded gradients 2) Lipchitz smoothness.
- Specifically studied the convergence of AdamW in the framework of $\alpha$-symmetric generalized-smooth functions, discussed different cases (interval of $\alpha$, with/without P≈Å condition) within the framework. Verified the partial resemblance to SignGD algorithm when $t\to \infty$
- Conducting experiments to verify the convergence results and the efficiency of the proposed selection of hyper-parameters and the dependance of certain parameters demonstrated in the theoretical results.
- Paper in preparation.
[[slides (for a pre during the internship)]](/files/r1.pdf)
